{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dew21zLoZbVA"
   },
   "source": [
    "# T-GCN  model\n",
    "\n",
    "We start with the baseline model (GCN and GRU) and try it to show how prediction works.\n",
    "\n",
    "\n",
    "Main idea of T-GCN:\n",
    "On one hand, the graph convolutional network is used to capture the topological structure of the urban road network to obtain the spatial dependence. On the other hand, the gated recurrent unit is used to capture the dynamic variation of traffic information on the roads to obtain the temporal dependence.\n",
    "\n",
    "### Measures to estimate predictions:\n",
    "R2 and Var calculate the correlation coefficient, which measures the ability of the prediction result to represent the actual data.\n",
    "\n",
    "\n",
    "### Plotting the dataset tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YS4Ag9X1Z8fg"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx\n",
    "\n",
    "# Load the dataset\n",
    "loader = ChickenpoxDatasetLoader()\n",
    "dataset = loader.get_dataset()\n",
    "\n",
    "# Extract target values (number of chickenpox cases) over time\n",
    "targets = [snapshot.y.numpy() for snapshot in dataset]\n",
    "\n",
    "# Plot the target values over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(targets[0].shape[0]):  # Iterate over each node\n",
    "    plt.plot([t[i] for t in targets], label=f'Node {i}')\n",
    "plt.title(\"Chickenpox Cases Over Time\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Number of Cases\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualize the graph structure for a specific snapshot (e.g., the first snapshot)\n",
    "snapshot = dataset[0]\n",
    "\n",
    "# Convert the PyTorch Geometric graph to a NetworkX graph\n",
    "G = to_networkx(snapshot, to_undirected=True)\n",
    "\n",
    "# Plot the graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "pos = nx.spring_layout(G)  # Layout for visualization\n",
    "nx.draw(G, pos, with_labels=True, node_color='lightblue', edge_color='gray', node_size=500, font_size=10)\n",
    "plt.title(\"Graph Structure at Time Step 0\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch_geometric_temporal.nn.recurrent import TGCN  # Temporal Graph Convolutional Network\n",
    "from torch_geometric_temporal.dataset import ChickenpoxDatasetLoader  # Dataset loader for chickenpox data\n",
    "from torch_geometric_temporal.signal import temporal_signal_split  # Utility to split temporal signals\n",
    "\n",
    "\n",
    "# Load the Chickenpox dataset\n",
    "loader = ChickenpoxDatasetLoader()\n",
    "dataset = loader.get_dataset()  # Get the dataset\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.2)\n",
    "\n",
    "# Define a Recurrent Graph Convolutional Network (GCN) model\n",
    "class RecurrentGCN(torch.nn.Module):\n",
    "\n",
    "  '''\n",
    "  first we apply GCN\n",
    "  then we apply GRU\n",
    "  '''\n",
    "    def __init__(self, node_features):\n",
    "        super(RecurrentGCN, self).__init__()\n",
    "        # TGCN layer: Temporal Graph Convolutional Network\n",
    "        self.recurrent = TGCN(node_features, 32)  # Input features, 32 hidden units\n",
    "        # Linear layer to map hidden state to output\n",
    "        self.linear = torch.nn.Linear(32, 1)  # 32 hidden units to 1 output unit\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight, prev_hidden_state):\n",
    "        # Apply the TGCN layer\n",
    "        h = self.recurrent(x, edge_index, edge_weight, prev_hidden_state)\n",
    "        # Apply ReLU activation\n",
    "        y = F.relu(h)\n",
    "        # Apply the linear layer to get the final output\n",
    "        y = self.linear(y)\n",
    "        return y, h  # Return output and hidden state\n",
    "\n",
    "# Initialize the model with 4 input node features\n",
    "model = RecurrentGCN(node_features=4)\n",
    "\n",
    "# Define the optimizer (Adam) with a learning rate of 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(150)):  # 150 epochs\n",
    "    cost = 0  # Initialize cost for the epoch\n",
    "    hidden_state = None  # Initialize hidden state\n",
    "    for time, snapshot in enumerate(train_dataset):  # Iterate over each snapshot in the training dataset\n",
    "        # Forward pass: compute predictions and hidden state\n",
    "        y_hat, hidden_state = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, hidden_state)\n",
    "        # Compute Mean Squared Error (MSE) loss\n",
    "        cost = cost + torch.mean((y_hat - snapshot.y) ** 2)\n",
    "    # Average the cost over all snapshots\n",
    "    cost = cost / (time + 1)\n",
    "    # Backpropagation\n",
    "    cost.backward()\n",
    "    # Update model parameters\n",
    "    optimizer.step()\n",
    "    # Clear gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Testing loop\n",
    "cost = 0  # Initialize cost for testing\n",
    "hidden_state = None  # Initialize hidden state\n",
    "for time, snapshot in enumerate(test_dataset):  # Iterate over each snapshot in the testing dataset\n",
    "    # Forward pass: compute predictions and hidden state\n",
    "    y_hat, hidden_state = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr, hidden_state)\n",
    "    # Compute Mean Squared Error (MSE) loss\n",
    "    cost = cost + torch.mean((y_hat - snapshot.y) ** 2)\n",
    "\n",
    "# Average the cost over all snapshots\n",
    "cost = cost / (time + 1)\n",
    "\n",
    "# Convert cost to a Python float\n",
    "cost = cost.item()\n",
    "\n",
    "# Print the Mean Squared Error (MSE) for the test dataset\n",
    "print(\"MSE: {:.4f}\".format(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txyZ-q4INkFP"
   },
   "source": [
    "## Method DFD - GCN\n",
    "\n",
    "Building the model\n",
    "\n",
    " Fourier Transform\n",
    "\n",
    "    Time-Shift Problem: The article states that the Time-Shift problem complicates the modeling of spatial dependencies in traffic data. To address this issue, the use of the Fourier Transform is proposed, which converts traffic data into the frequency domain.\n",
    "\n",
    "    Mathematical Justification:\n",
    "\n",
    "        Let f(t)f(t) be the traffic data captured by sensors at a specific intersection. If the traffic is delayed by time t0t0​, the data f(t−t0)f(t−t0​) will be captured at the next intersection.\n",
    "\n",
    "        According to the definition of the Fourier Transform:\n",
    "$$F(\\omega) = \\int_{-\\infty}^{\\infty} f(t) e^{-j\\omega t} dt$$\n",
    "\n",
    "$$F_{t_0}(\\omega) = \\int_{-\\infty}^{\\infty} f(t-t_0) e^{-j\\omega t} dt$$\n",
    "\n",
    "## Experiments\n",
    "\n",
    " Datasets: Experiments are conducted on four real-world datasets with tens of thousands of time steps and hundreds of sensors. The dataset statistics are presented in Table 1.\n",
    "\n",
    "## Basic Features and Metrics\n",
    "\n",
    "  Baselines and Metrics: Classical methods such as HI, GWNet, DCRNN, AGCRN, STGCN, MTGNN, DGCRN are chosen as baselines. Evaluation metrics include Mean Absolute Error (MAE), Root Mean Square Error (RMSE), and Mean Absolute Percentage Error (MAPE).\n",
    "\n",
    "  Experiment Settings: The datasets are divided into training, validation, and test sets in a 7:1:2 ratio. Traffic data is predicted for 12 time steps using historical data of length 12. The embedding sizes after Fourier Transform and identity are 10, and the time embeddings TtWTtW​ and TtDTtD​ are 12. The embedding size after 1D convolution is 30.\n",
    "\n",
    "  Experiment Results: DFDGCN shows better results compared to the baselines on all datasets. Ablation analysis confirms the effectiveness of the frequency graph in modeling dynamic spatial dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qIwUgq0rNkFU"
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pickle\n",
    "import folium\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vz2spbKtNkFZ"
   },
   "source": [
    "## Variation of the GCN model \n",
    "\n",
    "The variation of the model with embedding additional properties:\n",
    "- structural\n",
    "- temporal (time of the day, time of the week) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data \n",
    "\n",
    "\n",
    "We load the data from main repository here \n",
    "https://github.com/GestaltCogTeam/BasicTS/tree/master/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_0MvF3r9NkFg"
   },
   "outputs": [],
   "source": [
    "class convt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(convt, self).__init__()\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        x = torch.einsum('bne, ek->bnk', (x, w))\n",
    "        return x.contiguous()\n",
    "\n",
    "class nconv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(nconv, self).__init__()\n",
    "\n",
    "    def forward(self, x, A, dims):\n",
    "        if dims == 2:\n",
    "            x = torch.einsum('ncvl,vw->ncwl', (x, A))\n",
    "        elif dims == 3:\n",
    "            x = torch.einsum('ncvl,nvw->ncwl', (x, A))\n",
    "        else:\n",
    "            raise NotImplementedError('DFDGCN not implemented for A of dimension ' + str(dims))\n",
    "        return x.contiguous()\n",
    "\n",
    "class linear(nn.Module):\n",
    "    \"\"\"Linear layer.\"\"\"\n",
    "\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super(linear, self).__init__()\n",
    "        self.mlp = torch.nn.Conv2d(c_in, c_out, kernel_size=(\n",
    "            1, 1), padding=(0, 0), stride=(1, 1), bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class gcn(nn.Module):\n",
    "    \"\"\"Graph convolution network.\"\"\"\n",
    "\n",
    "    def __init__(self, c_in, c_out, dropout, support_len=3, order=2):\n",
    "        super(gcn, self).__init__()\n",
    "        self.nconv = nconv()\n",
    "\n",
    "        self.c_in = c_in\n",
    "        c_in = (order * (support_len + 1) + 1) * self.c_in\n",
    "        self.mlp = linear(c_in, c_out)\n",
    "        self.dropout = dropout\n",
    "        self.order = order\n",
    "\n",
    "    def forward(self, x, support):\n",
    "\n",
    "        out = [x]\n",
    "        for a in support:\n",
    "            x1 = self.nconv(x, a.to(x.device), a.dim())\n",
    "            out.append(x1)\n",
    "\n",
    "            for k in range(2, self.order + 1):\n",
    "                x2 = self.nconv(x1, a.to(x1.device), a.dim())\n",
    "                out.append(x2)\n",
    "                x1 = x2\n",
    "        h = torch.cat(out, dim=1)\n",
    "        h = self.mlp(h)\n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "        return h\n",
    "\n",
    "def dy_mask_graph(adj, k):\n",
    "    M = []\n",
    "    for i in range(adj.size(0)):\n",
    "        adp = adj[i]\n",
    "        mask = torch.zeros( adj.size(1),adj.size(2)).to(adj.device)\n",
    "        mask = mask.fill_(float(\"0\"))\n",
    "        s1, t1 = (adp + torch.rand_like(adp) * 0.01).topk(k, 1)\n",
    "        mask = mask.scatter_(1, t1, s1.fill_(1))\n",
    "        M.append(mask)\n",
    "    mask = torch.stack(M,dim=0)\n",
    "    adj = adj * mask\n",
    "    return adj\n",
    "\n",
    "def cat(x1,x2):\n",
    "    M = []\n",
    "    for i in range(x1.size(0)):\n",
    "        x = x1[i]\n",
    "        new_x = torch.cat([x,x2],dim=1)\n",
    "        M.append(new_x)\n",
    "    result = torch.stack(M,dim=0)\n",
    "    return result\n",
    "\n",
    "\n",
    "class DFDGCN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_nodes, dropout=0.3, supports=None,\n",
    "                    gcn_bool=True, addaptadj=True, aptinit=None,\n",
    "                    in_dim=2, out_dim=12, residual_channels=32,\n",
    "                    dilation_channels=32, skip_channels=256, end_channels=512,\n",
    "                    kernel_size=2, blocks=4, layers=2, a=1, seq_len=12, affine=True, fft_emb=10, identity_emb=10, hidden_emb=30, subgraph=20):\n",
    "        super(DFDGCN, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.blocks = blocks\n",
    "        self.layers = layers\n",
    "        self.gcn_bool = gcn_bool\n",
    "        self.addaptadj = addaptadj\n",
    "        self.filter_convs = nn.ModuleList()\n",
    "        self.gate_convs = nn.ModuleList()\n",
    "        self.residual_convs = nn.ModuleList()\n",
    "        self.skip_convs = nn.ModuleList()\n",
    "        self.bn = nn.ModuleList()\n",
    "        self.gconv = nn.ModuleList()\n",
    "        self.seq_len = seq_len\n",
    "        self.a = a\n",
    "\n",
    "        self.start_conv = nn.Conv2d(in_channels=in_dim,\n",
    "                                    out_channels=residual_channels,\n",
    "                                    kernel_size=(1, 1))\n",
    "\n",
    "        self.supports = supports\n",
    "        self.emb = fft_emb\n",
    "        self.subgraph_size = subgraph\n",
    "        self.identity_emb = identity_emb\n",
    "        self.hidden_emb = hidden_emb\n",
    "        self.fft_len = round(seq_len//2) + 1\n",
    "        self.Ex1 = nn.Parameter(torch.randn(self.fft_len, self.emb), requires_grad=True)\n",
    "        self.Wd = nn.Parameter(torch.randn(num_nodes,self.emb + self.identity_emb + self.seq_len * 2, self.hidden_emb), requires_grad=True)\n",
    "        self.Wxabs = nn.Parameter(torch.randn(self.hidden_emb, self.hidden_emb), requires_grad=True)\n",
    "\n",
    "        self.mlp = linear(residual_channels * 4,residual_channels)\n",
    "        self.layersnorm = torch.nn.LayerNorm(normalized_shape=[num_nodes,self.hidden_emb], eps=1e-08,elementwise_affine=affine)\n",
    "        self.convt = convt()\n",
    "\n",
    "        self.node1 = nn.Parameter(\n",
    "            torch.randn(num_nodes, self.identity_emb), requires_grad=True)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.T_i_D_emb = nn.Parameter(\n",
    "            torch.empty(288, self.seq_len))\n",
    "        self.D_i_W_emb = nn.Parameter(\n",
    "            torch.empty(7, self.seq_len))\n",
    "\n",
    "        receptive_field = 1\n",
    "        self.reset_parameter()\n",
    "        self.supports_len = 0\n",
    "        if not addaptadj:\n",
    "            self.supports_len -= 1\n",
    "        if supports is not None:\n",
    "            self.supports_len += len(supports)\n",
    "        if gcn_bool and addaptadj:\n",
    "            if aptinit is None:\n",
    "                if supports is None:\n",
    "                    self.supports = []\n",
    "                self.nodevec1 = nn.Parameter(\n",
    "                    torch.randn(num_nodes, self.emb), requires_grad=True)\n",
    "                self.nodevec2 = nn.Parameter(\n",
    "                    torch.randn(self.emb, num_nodes), requires_grad=True)\n",
    "                self.supports_len += 1\n",
    "            else:\n",
    "                if supports is None:\n",
    "                    self.supports = []\n",
    "                m, p, n = torch.svd(aptinit)\n",
    "                initemb1 = torch.mm(m[:, :10], torch.diag(p[:10] ** 0.5))\n",
    "                initemb2 = torch.mm(torch.diag(p[:10] ** 0.5), n[:, :10].t())\n",
    "                self.nodevec1 = nn.Parameter(initemb1, requires_grad=True)\n",
    "                self.nodevec2 = nn.Parameter(initemb2, requires_grad=True)\n",
    "                self.supports_len += 1\n",
    "\n",
    "        for b in range(blocks):\n",
    "            additional_scope = kernel_size - 1\n",
    "            new_dilation = 1\n",
    "            for i in range(layers):\n",
    "                # dilated convolutions\n",
    "                self.filter_convs.append(nn.Conv2d(in_channels=residual_channels,\n",
    "                                                   out_channels=dilation_channels,\n",
    "                                                   kernel_size=(1, kernel_size), dilation=new_dilation))\n",
    "\n",
    "                self.gate_convs.append(nn.Conv2d(in_channels=residual_channels,\n",
    "                                                 out_channels=dilation_channels,\n",
    "                                                 kernel_size=(1, kernel_size), dilation=new_dilation))\n",
    "\n",
    "                # 1x1 convolution for residual connection\n",
    "                self.residual_convs.append(nn.Conv2d(in_channels=dilation_channels,\n",
    "                                                     out_channels=residual_channels,\n",
    "                                                     kernel_size=(1, 1)))\n",
    "\n",
    "                # 1x1 convolution for skip connection\n",
    "                self.skip_convs.append(nn.Conv2d(in_channels=dilation_channels,\n",
    "                                                 out_channels=skip_channels,\n",
    "                                                 kernel_size=(1, 1)))\n",
    "                self.bn.append(nn.BatchNorm2d(residual_channels))\n",
    "                new_dilation *= 2\n",
    "                receptive_field += additional_scope\n",
    "                additional_scope *= 2\n",
    "                if self.gcn_bool:\n",
    "                    self.gconv.append(\n",
    "                        gcn(dilation_channels, residual_channels, dropout, support_len=self.supports_len))\n",
    "        self.end_conv_1 = nn.Conv2d(in_channels=skip_channels,\n",
    "                                    out_channels=end_channels,\n",
    "                                    kernel_size=(1, 1),\n",
    "                                    bias=True)\n",
    "\n",
    "        self.end_conv_2 = nn.Conv2d(in_channels=end_channels,\n",
    "                                    out_channels=out_dim,\n",
    "                                    kernel_size=(1, 1),\n",
    "                                    bias=True)\n",
    "\n",
    "        self.receptive_field = receptive_field\n",
    "\n",
    "    def reset_parameter(self):\n",
    "        nn.init.xavier_uniform_(self.T_i_D_emb)\n",
    "        nn.init.xavier_uniform_(self.D_i_W_emb)\n",
    "\n",
    "\n",
    "    def forward(self, history_data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Feedforward function of DFDGCN; Based on Graph WaveNet\n",
    "\n",
    "        Args:\n",
    "            history_data (torch.Tensor): shape [B, L, N, C]\n",
    "\n",
    "        Graphs:\n",
    "            predefined graphs: two graphs; [2, N, N] : Pre-given graph structure, including in-degree and out-degree graphs\n",
    "\n",
    "            self-adaptive graph: [N, N] : Self-Adaptively constructed graphs with two learnable parameters\n",
    "                torch.mm(self.nodevec1, self.nodevec2)\n",
    "                    nodevec: [N, Emb]\n",
    "\n",
    "            dynamic frequency domain graph: [B, N, N] : Data-driven graphs constructed with frequency domain information from traffic data\n",
    "                traffic_data : [B, N, L]\n",
    "                frequency domain information : [B, N, L/2.round + 1] ------Embedding ------[B, N, Emb2]\n",
    "                Identity embedding : learnable parameter [N, Emb3]\n",
    "                Time embedding : Week and Day : [N, 7] [N, 24(hour) * 12 (60min / 5min due to sampling)] ------Embedding ------ [N, 2 * Emb4]\n",
    "                Concat frequency domain information + Identity embedding + Time embedding ------Embedding , Activating, Normalization and Dropout\n",
    "                Conv1d to get adjacency matrix\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: [B, L, N, 1]\n",
    "        \"\"\"\n",
    "        #num_feat = model_args[\"num_feat\"]\n",
    "        input = history_data.transpose(1, 3).contiguous()[:,:,:,:]\n",
    "\n",
    "        data = history_data\n",
    "\n",
    "        in_len = input.size(3)\n",
    "        if in_len < self.receptive_field:\n",
    "            x = nn.functional.pad(\n",
    "                input, (self.receptive_field-in_len, 0, 0, 0))\n",
    "        else:\n",
    "            x = input\n",
    "        x = self.start_conv(x)\n",
    "\n",
    "        skip = 0\n",
    "        if self.gcn_bool and self.addaptadj and self.supports is not None:\n",
    "\n",
    "\n",
    "            gwadp = F.softmax(\n",
    "                F.relu(torch.mm(self.nodevec1, self.nodevec2)), dim=1)\n",
    "\n",
    "            new_supports = self.supports + [gwadp] # pretrained graph in DCRNN and self-adaptive graph in GWNet\n",
    "\n",
    "            # Construction of dynamic frequency domain graph\n",
    "            xn1 = input[:, 0, :, -self.seq_len:]\n",
    "\n",
    "            # T_D = self.T_i_D_emb[(data[:, :, :, 1] * 288).type(torch.LongTensor)][:, -1, :, :]\n",
    "            T_D = self.T_i_D_emb[(data[:, :, :, 1]).type(torch.LongTensor)][:, -1, :, :]\n",
    "            D_W = self.D_i_W_emb[(data[:, :, :, 1 + 1]).type(torch.LongTensor)][:, -1, :, :]\n",
    "\n",
    "            xn1 = torch.fft.rfft(xn1, dim=-1)\n",
    "            xn1 = torch.abs(xn1)\n",
    "\n",
    "            xn1 = torch.nn.functional.normalize(xn1, p=2.0, dim=1, eps=1e-12, out=None)\n",
    "            xn1 = torch.nn.functional.normalize(xn1, p=2.0, dim=2, eps=1e-12, out=None) * self.a\n",
    "\n",
    "            xn1 = torch.matmul(xn1, self.Ex1)\n",
    "            xn1k = cat(xn1, self.node1)\n",
    "            x_n1 = torch.cat([xn1k, T_D, D_W], dim=2)\n",
    "            x1 = torch.bmm(x_n1.permute(1,0,2),self.Wd).permute(1,0,2)\n",
    "            x1 = torch.relu(x1)\n",
    "            x1k = self.layersnorm(x1)\n",
    "            x1k = self.drop(x1k)\n",
    "            adp = self.convt(x1k, self.Wxabs)\n",
    "            adj = torch.bmm(adp, x1.permute(0, 2, 1))\n",
    "            adp = torch.relu(adj)\n",
    "            adp = dy_mask_graph(adp, self.subgraph_size)\n",
    "            adp = F.softmax(adp, dim=2)\n",
    "            new_supports = new_supports + [adp]\n",
    "\n",
    "        # WaveNet layers\n",
    "        for i in range(self.blocks * self.layers):\n",
    "\n",
    "            # dilated convolution\n",
    "            residual = x\n",
    "            filter = self.filter_convs[i](residual)\n",
    "            filter = torch.tanh(filter)\n",
    "            gate = self.gate_convs[i](residual)\n",
    "            gate = torch.sigmoid(gate)\n",
    "            x = filter * gate\n",
    "\n",
    "            # parametrized skip connection\n",
    "            s = x\n",
    "            s = self.skip_convs[i](s)\n",
    "            try:\n",
    "                skip = skip[:, :, :,  -s.size(3):]\n",
    "            except:\n",
    "                skip = 0\n",
    "            skip = s + skip\n",
    "\n",
    "            if self.gcn_bool and self.supports is not None:\n",
    "                if self.addaptadj:\n",
    "                    x = self.gconv[i](x, new_supports)\n",
    "\n",
    "                else:\n",
    "                    x = self.gconv[i](x, self.supports)\n",
    "            else:\n",
    "                x = self.residual_convs[i](x)\n",
    "            x = x + residual[:, :, :, -x.size(3):]\n",
    "\n",
    "            x = self.bn[i](x)\n",
    "\n",
    "        x = F.relu(skip)\n",
    "        x = F.relu(self.end_conv_1(x))\n",
    "        x = self.end_conv_2(x)\n",
    "        return x\n",
    "\n",
    "# model = DFDGCN(num_nodes=pems_bay_adj.shape[0], supports=supports, in_dim=combined_data.shape[3])\n",
    "\n",
    "# model(combined_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jGa3NlHnNkFi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Данные и параметры\n",
    "combined_data = combined_data_full[:, 288:2016+288, :, :]\n",
    "B, L, N, C = combined_data.shape  # [1, 52116, 325, 14]\n",
    "batch_size = 16\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "seq_len = 12  # Количество временных шагов на вход\n",
    "pred_len = 1  # Количество временных шагов для предсказания\n",
    "\n",
    "# Разделение данных на train, val и test\n",
    "num_samples = combined_data.shape[1]\n",
    "train_size = int(num_samples * train_ratio)\n",
    "val_size = int(num_samples * val_ratio)\n",
    "test_size = num_samples - train_size - val_size\n",
    "\n",
    "train_data = combined_data[:, :train_size, :, :]\n",
    "val_data = combined_data[:, train_size:train_size + val_size, :, :]\n",
    "test_data = combined_data[:, train_size + val_size:, :, :]\n",
    "\n",
    "\n",
    "# Создание кастомного Dataset\n",
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, data, seq_len, pred_len):\n",
    "        super().__init__()\n",
    "        self.data = data.squeeze(0)  # Убираем батч, форма [L, N, C]\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_len, :, :]  # Последовательность входных данных\n",
    "        y = self.data[idx + self.seq_len:idx + self.seq_len + self.pred_len, :, 0]  # Целевая скорость\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# Создание DataLoader\n",
    "train_dataset = TrafficDataset(train_data, seq_len, pred_len)\n",
    "val_dataset = TrafficDataset(val_data, seq_len, pred_len)\n",
    "test_dataset = TrafficDataset(test_data, seq_len, pred_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2rbgo6OYNkFj",
    "outputId": "f7078717-0780-4543-a4c6-b71cb0faaa6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Определение устройства\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else 'cpu'  # Устанавливаем CPU вместо CUDA\n",
    "print(device)\n",
    "\n",
    "# Обновление модели, данных и вычислений\n",
    "supports = [torch.tensor(pems_bay_adj.to_numpy(), dtype=torch.float32)]\n",
    "model = DFDGCN(num_nodes=N, supports=supports, in_dim=C, out_dim=pred_len).to(device)  # Модель на CPU\n",
    "criterion = nn.MSELoss()  # Функция потерь\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "d06ecb66e3e240ee8eca4339217ec095",
      "f1ebc9bbc23f4fdaae8ff10636ac75f4",
      "c857af85def04e3d9828282d69790b88"
     ]
    },
    "id": "bDdkKDc5NkFk",
    "outputId": "5cc3d3eb-321c-42bd-da10-b3c8522b20ca"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06ecb66e3e240ee8eca4339217ec095",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1 - Training:   0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1ebc9bbc23f4fdaae8ff10636ac75f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1 - Validation:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\romab\\AppData\\Local\\Temp\\ipykernel_10684\\4143650647.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c857af85def04e3d9828282d69790b88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 7.5325\n"
     ]
    }
   ],
   "source": [
    "# Тренировочный цикл\n",
    "def train_model(model, train_loader, val_loader, epochs):\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Тренировка\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} - Training\", leave=False)\n",
    "\n",
    "        for x, y in train_loader_tqdm:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x).squeeze(-1)  # Предсказание модели\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * x.size(0)\n",
    "\n",
    "            # Отображение среднего лосса\n",
    "            train_loader_tqdm.set_postfix({\"Batch Loss\": loss.item()})\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        # Валидация\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{epochs} - Validation\", leave=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader_tqdm:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                output = model(x).squeeze(-1)\n",
    "                loss = criterion(output, y)\n",
    "                val_loss += loss.item() * x.size(0)\n",
    "\n",
    "                # Отображение среднего лосса\n",
    "                val_loader_tqdm.set_postfix({\"Batch Loss\": loss.item()})\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "\n",
    "        # Сохранение лучшей модели\n",
    "        # tqdm.write(f\"Epoch {epoch + 1}/{epochs}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "\n",
    "# Тестирование\n",
    "def test_model(model, test_loader):\n",
    "    model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_loader_tqdm = tqdm(test_loader, desc=\"Testing\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader_tqdm:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output = model(x).squeeze(-1)\n",
    "            loss = criterion(output, y)\n",
    "            test_loss += loss.item() * x.size(0)\n",
    "\n",
    "            # Отображение среднего лосса\n",
    "            test_loader_tqdm.set_postfix({\"Batch Loss\": loss.item()})\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    tqdm.write(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "train_model(model, train_loader, val_loader, epochs=1)\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9PCRrDEqNkFm"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Данные и параметры\n",
    "combined_data = combined_data_full[:, 288:2016+288, :, :3]\n",
    "B, L, N, C = combined_data.shape  # [1, 52116, 325, 3]\n",
    "batch_size = 32\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "seq_len = 12  # Количество временных шагов на вход\n",
    "pred_len = 1  # Количество временных шагов для предсказания\n",
    "\n",
    "# Разделение данных на train, val и test\n",
    "num_samples = combined_data.shape[1]\n",
    "train_size = int(num_samples * train_ratio)\n",
    "val_size = int(num_samples * val_ratio)\n",
    "test_size = num_samples - train_size - val_size\n",
    "\n",
    "train_data = combined_data[:, :train_size, :, :]\n",
    "val_data = combined_data[:, train_size:train_size + val_size, :, :]\n",
    "test_data = combined_data[:, train_size + val_size:, :, :]\n",
    "\n",
    "\n",
    "# Создание кастомного Dataset\n",
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, data, seq_len, pred_len):\n",
    "        super().__init__()\n",
    "        self.data = data.squeeze(0)  # Убираем батч, форма [L, N, C]\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0] - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx:idx + self.seq_len, :, :]  # Последовательность входных данных\n",
    "        y = self.data[idx + self.seq_len:idx + self.seq_len + self.pred_len, :, 0]  # Целевая скорость\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# Создание DataLoader\n",
    "train_dataset = TrafficDataset(train_data, seq_len, pred_len)\n",
    "val_dataset = TrafficDataset(val_data, seq_len, pred_len)\n",
    "test_dataset = TrafficDataset(test_data, seq_len, pred_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BuCpCUNcNkFn"
   },
   "outputs": [],
   "source": [
    "# Определение устройства\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# Обновление модели, данных и вычислений\n",
    "supports = [torch.tensor(pems_bay_adj.to_numpy(), dtype=torch.float32)]\n",
    "model = DFDGCN(num_nodes=N, supports=supports, in_dim=C, out_dim=pred_len).to(device)\n",
    "criterion = nn.MSELoss()  # Функция потерь\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "dbf86860c38c401696bd27d6418f4a35",
      "31468f4d095442f597ed8c2bf5d50951",
      "c752bdf4ddf14961aeb29dd6884740b9"
     ]
    },
    "id": "SIyg97tFNkFn",
    "outputId": "84ca4a7c-f997-4a59-e503-55f8a48c6fd7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf86860c38c401696bd27d6418f4a35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1 - Training:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31468f4d095442f597ed8c2bf5d50951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/1 - Validation:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Local\\AppData\\Local\\Temp\\ipykernel_9196\\4143650647.py:50: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pth\"))\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c752bdf4ddf14961aeb29dd6884740b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 5.0635\n"
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, epochs=1)\n",
    "test_model(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
