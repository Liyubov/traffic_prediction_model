{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Добавляем путь на уровень выше\n",
    "sys.path.append(str(Path(os.getcwd()).resolve().parent))\n",
    "\n",
    "from utils.features import *\n",
    "from utils.load_data import load_all_data\n",
    "from utils.graph_features import GraphClusterProcessor\n",
    "from models.dfdgcn import DFDGCN\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../data/PEMS-BAY')\n",
    "metadata, data, adj = load_all_data(data_dir)\n",
    "data = data[:2016]\n",
    "data = data.copy()\n",
    "\n",
    "data[:, :, 1] = data[:, :, 1] * 288\n",
    "data[:, :, 2] = data[:, :, 2] * 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GraphClusterProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = GraphClusterProcessor(adj, data)\n",
    "processor.elbow_method(metric='closeness', max_clusters=15)\n",
    "\n",
    "labels = KMeans(n_clusters=10, random_state=42).fit_predict(\n",
    "    np.array(list(nx.betweenness_centrality(nx.from_numpy_array(adj)).values())).reshape(-1, 1)\n",
    ")\n",
    "processor.plot_group_average_speeds(labels, n_clusters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_expanded = processor.cluster_and_add_channel(metric='closeness', n_clusters=10, one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node Emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_indices = np.array([i for i in range(data.shape[1])])\n",
    "node_indices_expanded = np.expand_dims(node_indices, axis=0)  # Форма: (1, 325,)\n",
    "node_indices_expanded = np.expand_dims(node_indices_expanded, axis=-1)  # Форма: (1, 325, 1)\n",
    "node_indices_expanded = np.repeat(node_indices_expanded, data.shape[0], axis=0)  # Форма: (2016, 325, 1)\n",
    "\n",
    "# Объединение с временными рядами по последней оси\n",
    "data_expanded = np.concatenate([data, node_indices_expanded], axis=-1)  # Форма: (2016, 325, 3 + 1)\n",
    "\n",
    "# Проверка результата\n",
    "print(\"Исходные данные (временные ряды):\", data.shape)\n",
    "print(\"Эмбеддинги узлов:\", node_indices.shape)\n",
    "print(\"Объединённые данные:\", data_expanded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fill_diagonal(adj, 0)\n",
    "G = nx.from_numpy_array(adj)\n",
    "node2vec = Node2Vec(G, dimensions=64, walk_length=30, num_walks=400, p=1, q=1, workers=12)\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "node_embeddings = np.array([model.wv[str(node)] for node in G.nodes()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embeddings_expanded = np.expand_dims(node_embeddings, axis=0)  # Форма: (1, 325, 64)\n",
    "node_embeddings_expanded = np.repeat(node_embeddings_expanded, data.shape[0], axis=0)  # Форма: (2016, 325, 64)\n",
    "\n",
    "# 4. Объединение с временными рядами по последней оси\n",
    "data_expanded = np.concatenate([data, node_embeddings_expanded], axis=-1)  # Форма: (2016, 325, 3 + 64)\n",
    "\n",
    "# 5. Проверка результата\n",
    "print(\"Исходные данные (временные ряды):\", data.shape)\n",
    "print(\"Эмбеддинги узлов:\", node_embeddings.shape)\n",
    "print(\"Объединённые данные:\", data_expanded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position Encoding (PE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    position = np.arange(pos)[:, np.newaxis]\n",
    "    div_term = np.exp(np.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))\n",
    "    pe = np.zeros((pos, d_model))\n",
    "    pe[:, 0::2] = np.sin(position * div_term)\n",
    "    pe[:, 1::2] = np.cos(position * div_term)\n",
    "    return pe\n",
    "\n",
    "# Пример для 325 узлов\n",
    "num_nodes = data.shape[1]\n",
    "d_model = 16\n",
    "pe = positional_encoding(num_nodes, d_model)  # Форма: (325, 64)\n",
    "\n",
    "# Расширение до (batch_size, 325, 64)\n",
    "pe = positional_encoding(num_nodes, d_model).astype(np.float32)  # Используйте float32\n",
    "pe_expanded = np.expand_dims(pe, axis=0)  # Форма: (1, 325, 64)\n",
    "pe_expanded = np.repeat(pe_expanded, data.shape[0], axis=0)  # Форма: (2016, 325, 64)\n",
    "pe_expanded = torch.tensor(pe_expanded, dtype=torch.float32)     # Для тензоров\n",
    "\n",
    "data_expanded = np.concatenate([data, pe_expanded], axis=-1)  # Форма: (2016, 325, 3 + 64)\n",
    "\n",
    "# 5. Проверка результата\n",
    "print(\"Исходные данные (временные ряды):\", data.shape)\n",
    "print(\"Эмбеддинги узлов:\", pe_expanded.shape)\n",
    "print(\"Объединённые данные:\", data_expanded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разделение данных и создание Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные и параметры\n",
    "L, N, C = data_expanded.shape  # [2016, 325, C]\n",
    "batch_size = 16\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.2\n",
    "seq_len = 12  # Количество временных шагов на вход\n",
    "pred_len = 12  # Количество временных шагов для предсказания\n",
    "\n",
    "# Индексы каналов для нормализации\n",
    "normalize = True\n",
    "\n",
    "channels_to_normalize = [0]\n",
    "\n",
    "# Проверка корректности разделения данных\n",
    "assert train_ratio + val_ratio + test_ratio == 1.0, \"Сумма долей train, val и test должна быть равна 1.0\"\n",
    "\n",
    "# Разделение данных на train, val и test\n",
    "num_samples = data_expanded.shape[0]  # Количество временных шагов (L)\n",
    "train_size = int(num_samples * train_ratio)\n",
    "val_size = int(num_samples * val_ratio)\n",
    "test_size = num_samples - train_size - val_size\n",
    "\n",
    "train_data = data_expanded[:train_size, :, :]  # [train_size, N, C]\n",
    "val_data = data_expanded[train_size:train_size + val_size, :, :]  # [val_size, N, C]\n",
    "test_data = data_expanded[train_size + val_size:, :, :]  # [test_size, N, C]\n",
    "\n",
    "# Нормализация данных\n",
    "if normalize:\n",
    "    assert all(0 <= ch < C for ch in channels_to_normalize), \"Индексы каналов выходят за пределы допустимого диапазона\"\n",
    "    channel_max = train_data[:, :, channels_to_normalize].max(axis=(0, 1), keepdims=True)  # Форма [1, 1, len(channels_to_normalize)]\n",
    "    channel_max[channel_max == 0] = 1.0\n",
    "    train_data[:, :, channels_to_normalize] = train_data[:, :, channels_to_normalize] / channel_max\n",
    "    val_data[:, :, channels_to_normalize] = val_data[:, :, channels_to_normalize] / channel_max\n",
    "    test_data[:, :, channels_to_normalize] = test_data[:, :, channels_to_normalize] / channel_max\n",
    "\n",
    "# Создание кастомного Dataset\n",
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, data, seq_len, pred_len):\n",
    "        super().__init__()\n",
    "        self.data = data  # Форма [L, N, C]\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "    def __len__(self):\n",
    "        # Количество возможных последовательностей\n",
    "        return self.data.shape[0] - self.seq_len - self.pred_len + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Извлекаем последовательность входных данных\n",
    "        x = self.data[idx:idx + self.seq_len, :, :]  # Форма [seq_len, N, C]\n",
    "        # Извлекаем целевую последовательность\n",
    "        y = self.data[idx + self.seq_len:idx + self.seq_len + self.pred_len, :, 0]  # Форма [pred_len, N, C]\n",
    "        return x, y\n",
    "    \n",
    "# Преобразование данных в тензоры с dtype=torch.float32\n",
    "train_data = torch.tensor(train_data, dtype=torch.float32)\n",
    "val_data = torch.tensor(val_data, dtype=torch.float32)\n",
    "test_data = torch.tensor(test_data, dtype=torch.float32)\n",
    "\n",
    "# Создание DataLoader\n",
    "train_dataset = TrafficDataset(train_data, seq_len, pred_len)\n",
    "val_dataset = TrafficDataset(val_data, seq_len, pred_len)\n",
    "test_dataset = TrafficDataset(test_data, seq_len, pred_len)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "    print(x[0, 0, :2, :])\n",
    "    print(y[:2, :2, :2])\n",
    "    print(x[0, 0, 0, 0].dtype)\n",
    "    print(x[0, 0, 0, 1].dtype)\n",
    "    print(x[0, 0, 0, 2].dtype)\n",
    "    print(x[0, 0, 0, 3].dtype)\n",
    "    1/0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация моделей и запуск обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QGNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "def normalize_directed_adj(adj, mode='row'):\n",
    "    \"\"\"\n",
    "    Нормализация для ориентированного графа.\n",
    "    \n",
    "    Args:\n",
    "        adj (torch.Tensor): Матрица смежности (N x N).\n",
    "        mode (str): 'row' для нормализации по строкам, 'col' для нормализации по столбцам.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Нормализованная матрица смежности.\n",
    "    \"\"\"\n",
    "    adj = adj + torch.eye(adj.size(0)).to(adj.device)\n",
    "    if mode == 'row':\n",
    "        degree = torch.sum(adj, dim=1, keepdim=True)\n",
    "    elif mode == 'col':\n",
    "        degree = torch.sum(adj, dim=0, keepdim=True)\n",
    "    else:\n",
    "        raise ValueError(\"Mode must be 'row' or 'col'.\")\n",
    "    \n",
    "    degree_inv = torch.where(degree > 0, 1.0 / degree, torch.zeros_like(degree))\n",
    "    return adj * degree_inv\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"\n",
    "    Нормализует матрицу смежности симметричным способом.\n",
    "    \n",
    "    Args:\n",
    "        adj (torch.Tensor): Матрица смежности (N x N).\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Нормализованная матрица смежности.\n",
    "    \"\"\"\n",
    "    # Добавляем петли (self-loops) к матрице смежности\n",
    "    adj = adj + torch.eye(adj.size(0)).to(adj.device)\n",
    "    \n",
    "    # Вычисляем степени узлов (degree matrix)\n",
    "    degree = torch.sum(adj, dim=1)\n",
    "    \n",
    "    # Симметричная нормализация: D^(-1/2) * A * D^(-1/2)\n",
    "    degree_inv_sqrt = torch.diag(torch.pow(degree, -0.5))\n",
    "    adj_normalized = torch.mm(torch.mm(degree_inv_sqrt, adj), degree_inv_sqrt)\n",
    "    \n",
    "    return adj_normalized\n",
    "\n",
    "# Функция для создания матрицы Гамильтона\n",
    "def make_quaternion_mul(kernel):\n",
    "    dim = kernel.size(1) // 4\n",
    "    r, i, j, k = torch.split(kernel, [dim, dim, dim, dim], dim=1)\n",
    "    hamilton = torch.cat([\n",
    "        torch.cat([r, -i, -j, -k], dim=0),\n",
    "        torch.cat([i, r, -k, j], dim=0),\n",
    "        torch.cat([j, k, r, -i], dim=0),\n",
    "        torch.cat([k, -j, i, r], dim=0)\n",
    "    ], dim=1)\n",
    "    return hamilton\n",
    "\n",
    "\n",
    "class QGNNLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, num_nodes, space, dropout=0.5, act=F.tanh):\n",
    "        super(QGNNLayer, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.space = space\n",
    "        self.act = act\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.bn = nn.BatchNorm1d(out_features)\n",
    "\n",
    "        if self.space == 'q':\n",
    "            assert in_features % 4 == 0, \"in_features must be divisible by 4 for quaternion operations\"\n",
    "            self.weight = Parameter(torch.FloatTensor(in_features // 4, out_features))\n",
    "        else:\n",
    "            self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "\n",
    "        self.dim = self.weight.size(1) // 4\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = math.sqrt(6.0 / (self.weight.size(0) + self.weight.size(1)))\n",
    "        self.weight.data.uniform_(-stdv, stdv)\n",
    "    \n",
    "    def forward(self, x, adj):\n",
    "        x = self.dropout(x)\n",
    "        if self.space == 'q':\n",
    "            hamilton = make_quaternion_mul(self.weight)\n",
    "            support = torch.mm(x, hamilton)  # Hamilton product\n",
    "        else:\n",
    "            support = torch.mm(x, self.weight)\n",
    "      \n",
    "        B_N, hidden_dim = support.shape\n",
    "        B = B_N // self.num_nodes\n",
    "        N = self.num_nodes\n",
    "        support = support.view(B, N, hidden_dim)\n",
    "        # Обработка каждого батча с использованием torch.bmm\n",
    "        output = torch.bmm(adj.unsqueeze(0).expand(B, -1, -1), support)\n",
    "        output = output.view(B * N, hidden_dim)\n",
    "        output = self.bn(output)\n",
    "        output = self.act(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "class QGNNTrafficPredictor(nn.Module):\n",
    "    def __init__(self, adj, num_nodes, input_dim, hidden_dim, output_dim, num_layers, pre_len, space, emb_configs, dropout=0.5, directed=False):\n",
    "        \"\"\"\n",
    "        adj: матрица смежности\n",
    "        num_nodes: количество узлов\n",
    "        input_dim: размерность входных данных (без учёта эмбеддингов)\n",
    "        hidden_dim: размер скрытого слоя\n",
    "        output_dim: размер выходного слоя\n",
    "        num_layers: количество GNN-слоёв\n",
    "        pre_len: длина предсказания\n",
    "        space: 'q' или 'r' (quaternion или обычные)\n",
    "        emb_configs: словарь {канал: (num_embeddings, embedding_dim)}\n",
    "        dropout: коэффициент дропаута\n",
    "        \"\"\"\n",
    "        super(QGNNTrafficPredictor, self).__init__()\n",
    "        self.adj = normalize_directed_adj(adj) if directed else normalize_adj(adj)\n",
    "        self.pre_len = pre_len\n",
    "        self.num_nodes = num_nodes\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Инициализация эмбеддингов\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            f\"emb_{channel}\": nn.Embedding(num_embeddings, emb_dim)\n",
    "            for channel, (num_embeddings, emb_dim) in emb_configs.items()\n",
    "        })\n",
    "\n",
    "        total_emb_dim = sum(emb_dim for _, (_, emb_dim) in emb_configs.items())\n",
    "\n",
    "        # QGNN слои\n",
    "        self.qgnn_layers = nn.ModuleList([\n",
    "            QGNNLayer(input_dim if i == 0 else hidden_dim, hidden_dim, self.num_nodes, space, dropout)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # GRU\n",
    "        self.temporal_layer = nn.GRU(hidden_dim + total_emb_dim + 1, hidden_dim, batch_first=True)\n",
    "        self.relu = F.relu\n",
    "\n",
    "        # FC\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, history_data, removed_channel=None):\n",
    "        \"\"\"\n",
    "        history_data: [B, L, N, C] - тензор временных рядов (где некоторые каналы содержат индексы эмбеддингов)\n",
    "        removed_channel: индекс канала, который нужно исключить\n",
    "        \"\"\"\n",
    "        B, L, N, C = history_data.shape\n",
    "\n",
    "        # Создание маски для исключения канала\n",
    "        if removed_channel is not None:\n",
    "            mask = torch.ones(C).to(history_data.device)\n",
    "            mask[removed_channel] = 0  # Отключаем канал\n",
    "            history_data = history_data * mask  # Применяем маску к данным\n",
    "\n",
    "        history_data = history_data.permute(0, 2, 1, 3)  # [B, N, L, C]\n",
    "        speed = history_data[:, :, :, :1]  # [B, N, L, 1]\n",
    "\n",
    "        # Обрабатываем эмбеддинги, используя индексы из history_data\n",
    "        emb_list = []\n",
    "        for channel, emb_layer in self.embeddings.items():\n",
    "            channel_idx = int(channel.split(\"_\")[1])  # Получаем номер канала\n",
    "            emb_indices = history_data[:, :, :, channel_idx].long()  # [B, N, L]\n",
    "            emb_list.append(emb_layer(emb_indices))  # [B, N, L, emb_dim]\n",
    "\n",
    "        emb_concat = torch.cat(emb_list, dim=-1) if emb_list else None  # [B, N, L, total_emb_dim]\n",
    "\n",
    "        outputs = []\n",
    "        for t in range(L):\n",
    "            x = history_data[:, :, t, :1]  # [B, N, 1]\n",
    "            x = x.reshape(B * N, -1)  # [B * N, 1]\n",
    "\n",
    "            for qgnn_layer in self.qgnn_layers:\n",
    "                x = qgnn_layer(x, self.adj)  # [B * N, hidden_dim]\n",
    "\n",
    "            x = x.reshape(B, N, -1)  # [B, N, hidden_dim]\n",
    "            outputs.append(x)\n",
    "\n",
    "        outputs = torch.stack(outputs, dim=2)  # [B, N, L, hidden_dim]\n",
    "\n",
    "        # Добавляем эмбеддинги\n",
    "        if emb_concat is not None:\n",
    "            outputs = torch.cat([speed, outputs, emb_concat], dim=-1)  # [B, N, L, hidden_dim + total_emb_dim]\n",
    "\n",
    "        # Применяем GRU\n",
    "        outputs = outputs.reshape(B * N, L, -1)  # [B * N, L, hidden_dim + total_emb_dim]\n",
    "        outputs, _ = self.temporal_layer(outputs)  # [B * N, L, hidden_dim]\n",
    "        outputs = outputs.reshape(B, N, L, -1)  # [B, N, L, hidden_dim]\n",
    "        outputs = outputs[:, :, -self.pre_len:, :]  # [B, N, pre_len, hidden_dim]\n",
    "        outputs = self.relu(outputs)\n",
    "\n",
    "        # Выходной слой\n",
    "        outputs = self.fc(outputs)  # [B, N, pre_len, output_dim]\n",
    "        return outputs.permute(0, 2, 1, 3)  # [B, pre_len, N, output_dim]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DFDGCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embed_size, heads):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.heads = heads\n",
    "        self.head_dim = embed_size // heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * heads == embed_size\n",
    "        ), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
    "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
    "\n",
    "    def forward(self, values, keys, query):\n",
    "        N = query.shape[0]\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
    "\n",
    "        # Split the embedding into self.heads different pieces\n",
    "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
    "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
    "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
    "\n",
    "        values = self.values(values)\n",
    "        keys = self.keys(keys)\n",
    "        queries = self.queries(queries)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        attention = torch.softmax(energy / (self.embed_size ** (1 / 2)), dim=3)\n",
    "\n",
    "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
    "            N, query_len, self.heads * self.head_dim\n",
    "        )\n",
    "\n",
    "        out = self.fc_out(out)\n",
    "        return out\n",
    "\n",
    "class convt(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(convt, self).__init__()\n",
    "\n",
    "    def forward(self, x, w):\n",
    "        x = torch.einsum('bne, ek->bnk', (x, w))\n",
    "        return x.contiguous()\n",
    "    \n",
    "class nconv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(nconv, self).__init__()\n",
    "\n",
    "    def forward(self, x, A, dims):\n",
    "        if dims == 2:\n",
    "            x = torch.einsum('ncvl,vw->ncwl', (x, A))\n",
    "        elif dims == 3:\n",
    "            x = torch.einsum('ncvl,nvw->ncwl', (x, A))\n",
    "        else:\n",
    "            raise NotImplementedError('DFDGCN not implemented for A of dimension ' + str(dims))\n",
    "        return x.contiguous()\n",
    "\n",
    "class linear(nn.Module):\n",
    "    \"\"\"Linear layer.\"\"\"\n",
    "\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super(linear, self).__init__()\n",
    "        self.mlp = torch.nn.Conv2d(c_in, c_out, kernel_size=(\n",
    "            1, 1), padding=(0, 0), stride=(1, 1), bias=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class gcn(nn.Module):\n",
    "    \"\"\"Graph convolution network.\"\"\"\n",
    "\n",
    "    def __init__(self, c_in, c_out, dropout, support_len=3, order=2):\n",
    "        super(gcn, self).__init__()\n",
    "        self.nconv = nconv()\n",
    "\n",
    "        self.c_in = c_in\n",
    "        c_in = (order * (support_len + 1) + 1) * self.c_in\n",
    "        self.mlp = linear(c_in, c_out)\n",
    "        self.dropout = dropout\n",
    "        self.order = order\n",
    "\n",
    "    def forward(self, x, support):\n",
    "\n",
    "        out = [x]\n",
    "        for a in support:\n",
    "            x1 = self.nconv(x, a.to(x.device), a.dim())\n",
    "            out.append(x1)\n",
    "\n",
    "            for k in range(2, self.order + 1):\n",
    "                x2 = self.nconv(x1, a.to(x1.device), a.dim())\n",
    "                out.append(x2)\n",
    "                x1 = x2\n",
    "        h = torch.cat(out, dim=1)\n",
    "        h = self.mlp(h)\n",
    "        h = F.dropout(h, self.dropout, training=self.training)\n",
    "        return h\n",
    "\n",
    "def dy_mask_graph(adj, k):\n",
    "    M = []\n",
    "    for i in range(adj.size(0)):\n",
    "        adp = adj[i]\n",
    "        mask = torch.zeros( adj.size(1),adj.size(2)).to(adj.device)\n",
    "        mask = mask.fill_(float(\"0\"))\n",
    "        s1, t1 = (adp + torch.rand_like(adp) * 0.01).topk(k, 1)\n",
    "        mask = mask.scatter_(1, t1, s1.fill_(1))\n",
    "        M.append(mask)\n",
    "    mask = torch.stack(M,dim=0)\n",
    "    adj = adj * mask\n",
    "    return adj\n",
    "\n",
    "def cat(x1,x2):\n",
    "    M = []\n",
    "    for i in range(x1.size(0)):\n",
    "        x = x1[i]\n",
    "        new_x = torch.cat([x,x2],dim=1)\n",
    "        M.append(new_x)\n",
    "    result = torch.stack(M,dim=0)\n",
    "    return result\n",
    "\n",
    "class DFDGCN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_nodes, dropout=0.3, supports=None,\n",
    "                    gcn_bool=True, addaptadj=True, aptinit=None,\n",
    "                    in_dim=2, out_dim=12, residual_channels=32,\n",
    "                    dilation_channels=32, skip_channels=256, end_channels=512,\n",
    "                    kernel_size=2, blocks=4, layers=2, a=1, seq_len=12, affine=True, \n",
    "                    fft_emb=10, identity_emb=10, hidden_emb=30, subgraph=20, \n",
    "                    add_channels=0, embed_size=64, heads=4):\n",
    "        super(DFDGCN, self).__init__()\n",
    "        self.a = a\n",
    "        self.heads = heads\n",
    "        self.emb = fft_emb\n",
    "        self.blocks = blocks\n",
    "        self.layers = layers\n",
    "        self.seq_len = seq_len\n",
    "        self.dropout = dropout\n",
    "        self.supports = supports\n",
    "        self.gcn_bool = gcn_bool\n",
    "        self.addaptadj = addaptadj\n",
    "        self.hidden_emb = hidden_emb\n",
    "        self.embed_size = embed_size\n",
    "        self.subgraph_size = subgraph\n",
    "        self.add_channels = add_channels\n",
    "        self.identity_emb = identity_emb\n",
    "        self.bn = nn.ModuleList()\n",
    "        self.gconv = nn.ModuleList()\n",
    "        self.gate_convs = nn.ModuleList()\n",
    "        self.skip_convs = nn.ModuleList()\n",
    "        self.filter_convs = nn.ModuleList()\n",
    "        self.residual_convs = nn.ModuleList()\n",
    "        self.self_attention = SelfAttention(embed_size, heads)\n",
    "        self.start_conv = nn.Conv2d(in_channels=in_dim,\n",
    "                                    out_channels=residual_channels,\n",
    "                                    kernel_size=(1, 1))\n",
    "\n",
    "        self.fft_len = round(seq_len//2) + 1\n",
    "        self.Ex1 = nn.Parameter(torch.randn(self.fft_len, self.emb), requires_grad=True)\n",
    "        self.Wd = nn.Parameter(torch.randn(num_nodes,self.emb + self.identity_emb + self.seq_len * 2 + self.add_channels, self.hidden_emb), requires_grad=True)\n",
    "        self.Wxabs = nn.Parameter(torch.randn(self.hidden_emb, self.hidden_emb), requires_grad=True)\n",
    "        self.pe = nn.Parameter(torch.randn(embed_size, embed_size), requires_grad=True)\n",
    "\n",
    "        self.mlp = linear(residual_channels * 4,residual_channels)\n",
    "        self.layersnorm = torch.nn.LayerNorm(normalized_shape=[num_nodes,self.hidden_emb], eps=1e-08,elementwise_affine=affine)\n",
    "        self.convt = convt()\n",
    "\n",
    "        self.node1 = nn.Parameter(\n",
    "            torch.randn(num_nodes, self.identity_emb), requires_grad=True)\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.T_i_D_emb = nn.Parameter(\n",
    "            torch.empty(288, self.seq_len))\n",
    "        self.D_i_W_emb = nn.Parameter(\n",
    "            torch.empty(7, self.seq_len))\n",
    "        self.G_emb = nn.Parameter(\n",
    "            torch.empty(num_nodes, self.seq_len))\n",
    "\n",
    "        receptive_field = 1\n",
    "        self.reset_parameter()\n",
    "        self.supports_len = 0\n",
    "        if not addaptadj:\n",
    "            self.supports_len -= 1\n",
    "        if supports is not None:\n",
    "            self.supports_len += len(supports)\n",
    "        if gcn_bool and addaptadj:\n",
    "            if aptinit is None:\n",
    "                if supports is None:\n",
    "                    self.supports = []\n",
    "                self.nodevec1 = nn.Parameter(\n",
    "                    torch.randn(num_nodes, self.emb), requires_grad=True)\n",
    "                self.nodevec2 = nn.Parameter(\n",
    "                    torch.randn(self.emb, num_nodes), requires_grad=True)\n",
    "                self.supports_len += 1\n",
    "            else:\n",
    "                if supports is None:\n",
    "                    self.supports = []\n",
    "                m, p, n = torch.svd(aptinit)\n",
    "                initemb1 = torch.mm(m[:, :10], torch.diag(p[:10] ** 0.5))\n",
    "                initemb2 = torch.mm(torch.diag(p[:10] ** 0.5), n[:, :10].t())\n",
    "                self.nodevec1 = nn.Parameter(initemb1, requires_grad=True)\n",
    "                self.nodevec2 = nn.Parameter(initemb2, requires_grad=True)\n",
    "                self.supports_len += 1\n",
    "\n",
    "        for b in range(blocks):\n",
    "            additional_scope = kernel_size - 1\n",
    "            new_dilation = 1\n",
    "            for i in range(layers):\n",
    "                # dilated convolutions\n",
    "                self.filter_convs.append(nn.Conv2d(in_channels=residual_channels,\n",
    "                                                   out_channels=dilation_channels,\n",
    "                                                   kernel_size=(1, kernel_size), dilation=new_dilation))\n",
    "\n",
    "                self.gate_convs.append(nn.Conv2d(in_channels=residual_channels,\n",
    "                                                 out_channels=dilation_channels,\n",
    "                                                 kernel_size=(1, kernel_size), dilation=new_dilation))\n",
    "\n",
    "                # 1x1 convolution for residual connection\n",
    "                self.residual_convs.append(nn.Conv2d(in_channels=dilation_channels,\n",
    "                                                     out_channels=residual_channels,\n",
    "                                                     kernel_size=(1, 1)))\n",
    "\n",
    "                # 1x1 convolution for skip connection\n",
    "                self.skip_convs.append(nn.Conv2d(in_channels=dilation_channels,\n",
    "                                                 out_channels=skip_channels,\n",
    "                                                 kernel_size=(1, 1)))\n",
    "                self.bn.append(nn.BatchNorm2d(residual_channels))\n",
    "                new_dilation *= 2\n",
    "                receptive_field += additional_scope\n",
    "                additional_scope *= 2\n",
    "                if self.gcn_bool:\n",
    "                    self.gconv.append(\n",
    "                        gcn(dilation_channels, residual_channels, dropout, support_len=self.supports_len))\n",
    "        self.end_conv_1 = nn.Conv2d(in_channels=skip_channels,\n",
    "                                    out_channels=end_channels,\n",
    "                                    kernel_size=(1, 1),\n",
    "                                    bias=True)\n",
    "\n",
    "        self.end_conv_2 = nn.Conv2d(in_channels=end_channels,\n",
    "                                    out_channels=out_dim,\n",
    "                                    kernel_size=(1, 1),\n",
    "                                    bias=True)\n",
    "\n",
    "        self.receptive_field = receptive_field\n",
    "\n",
    "    def reset_parameter(self):\n",
    "        nn.init.xavier_uniform_(self.T_i_D_emb)\n",
    "        nn.init.xavier_uniform_(self.D_i_W_emb)\n",
    "        nn.init.xavier_uniform_(self.G_emb)\n",
    "\n",
    "\n",
    "    def forward(self, history_data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Feedforward function of DFDGCN; Based on Graph WaveNet\n",
    "\n",
    "        Args:\n",
    "            history_data (torch.Tensor): shape [B, L, N, C]\n",
    "\n",
    "        Graphs:\n",
    "            predefined graphs: two graphs; [2, N, N] : Pre-given graph structure, including in-degree and out-degree graphs\n",
    "\n",
    "            self-adaptive graph: [N, N] : Self-Adaptively constructed graphs with two learnable parameters\n",
    "                torch.mm(self.nodevec1, self.nodevec2)\n",
    "                    nodevec: [N, Emb]\n",
    "\n",
    "            dynamic frequency domain graph: [B, N, N] : Data-driven graphs constructed with frequency domain information from traffic data\n",
    "                traffic_data : [B, N, L]\n",
    "                frequency domain information : [B, N, L/2.round + 1] ------Embedding ------[B, N, Emb2]\n",
    "                Identity embedding : learnable parameter [N, Emb3]\n",
    "                Time embedding : Week and Day : [N, 7] [N, 24(hour) * 12 (60min / 5min due to sampling)] ------Embedding ------ [N, 2 * Emb4]\n",
    "                Concat frequency domain information + Identity embedding + Time embedding ------Embedding , Activating, Normalization and Dropout\n",
    "                Conv1d to get adjacency matrix\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: [B, L, N, 1]\n",
    "        \"\"\"\n",
    "        #num_feat = model_args[\"num_feat\"]\n",
    "        input = history_data.transpose(1, 3).contiguous()[:,:,:,:]\n",
    "        \n",
    "        data = history_data\n",
    "\n",
    "\n",
    "        in_len = input.size(3)\n",
    "        if in_len < self.receptive_field:\n",
    "            x = nn.functional.pad(\n",
    "                input, (self.receptive_field-in_len, 0, 0, 0))\n",
    "        else:\n",
    "            x = input\n",
    "        x = self.start_conv(x)\n",
    "\n",
    "        skip = 0\n",
    "        if self.gcn_bool and self.addaptadj and self.supports is not None:\n",
    "\n",
    "\n",
    "            gwadp = F.softmax(\n",
    "                F.relu(torch.mm(self.nodevec1, self.nodevec2)), dim=1)\n",
    "\n",
    "            new_supports = self.supports + [gwadp] # pretrained graph in DCRNN and self-adaptive graph in GWNet\n",
    "\n",
    "            # Construction of dynamic frequency domain graph\n",
    "            xn1 = input[:, 0, :, -self.seq_len:]\n",
    "\n",
    "            T_D = self.T_i_D_emb[(data[:, :, :, 1] * 288).type(torch.LongTensor)][:, -1, :, :]\n",
    "            D_W = self.D_i_W_emb[(data[:, :, :, 2] * 7).type(torch.LongTensor)][:, -1, :, :]\n",
    "            # G_E = self.G_emb[(data[:, :, :, 3]).type(torch.LongTensor)][:, -1, :, :]\n",
    "            # node2vec_emb = data[:, -1, :, 3:]\n",
    "            # attended_emb = self.self_attention(node2vec_emb, node2vec_emb, node2vec_emb)\n",
    "            pe_emb = data[:, -1, :, -64:]  # Форма: (B, N, 64)\n",
    "            pe_emb = torch.matmul(pe_emb, self.pe)\n",
    "            # attended_emb = self.self_attention(pe_emb, pe_emb, pe_emb)\n",
    "            # pe_expanded = self.pe.unsqueeze(0).repeat(xn1.size(0), 1, 1)  # Расширяем PE до (B, N, 64)\n",
    "\n",
    "            xn1 = torch.fft.rfft(xn1, dim=-1)\n",
    "            xn1 = torch.abs(xn1)\n",
    "\n",
    "            xn1 = torch.nn.functional.normalize(xn1, p=2.0, dim=1, eps=1e-12, out=None)\n",
    "            xn1 = torch.nn.functional.normalize(xn1, p=2.0, dim=2, eps=1e-12, out=None) * self.a\n",
    "\n",
    "            xn1 = torch.matmul(xn1, self.Ex1)\n",
    "\n",
    "            xn1k = cat(xn1, self.node1)\n",
    "            x_n1 = torch.cat([xn1k, T_D, D_W, pe_emb], dim=2)\n",
    "            x1 = torch.bmm(x_n1.permute(1,0,2),self.Wd).permute(1,0,2)\n",
    "            x1 = torch.relu(x1)\n",
    "            x1k = self.layersnorm(x1)\n",
    "            x1k = self.drop(x1k)\n",
    "            adp = self.convt(x1k, self.Wxabs)\n",
    "            adj = torch.bmm(adp, x1.permute(0, 2, 1))\n",
    "            adp = torch.relu(adj)\n",
    "            adp = dy_mask_graph(adp, self.subgraph_size)\n",
    "            adp = F.softmax(adp, dim=2)\n",
    "            new_supports = new_supports + [adp]\n",
    "\n",
    "        # WaveNet layers\n",
    "        for i in range(self.blocks * self.layers):\n",
    "\n",
    "            # dilated convolution\n",
    "            residual = x\n",
    "            filter = self.filter_convs[i](residual)\n",
    "            filter = torch.tanh(filter)\n",
    "            gate = self.gate_convs[i](residual)\n",
    "            gate = torch.sigmoid(gate)\n",
    "            x = filter * gate\n",
    "\n",
    "            # parametrized skip connection\n",
    "            s = x\n",
    "            s = self.skip_convs[i](s)\n",
    "            try:\n",
    "                skip = skip[:, :, :,  -s.size(3):]\n",
    "            except:\n",
    "                skip = 0\n",
    "            skip = s + skip\n",
    "\n",
    "            if self.gcn_bool and self.supports is not None:\n",
    "                if self.addaptadj:\n",
    "                    x = self.gconv[i](x, new_supports)\n",
    "\n",
    "                else:\n",
    "                    x = self.gconv[i](x, self.supports)\n",
    "            else:\n",
    "                x = self.residual_convs[i](x)\n",
    "            x = x + residual[:, :, :, -x.size(3):]\n",
    "\n",
    "            x = self.bn[i](x)\n",
    "\n",
    "        x = F.relu(skip)\n",
    "        x = F.relu(self.end_conv_1(x))\n",
    "        x = self.end_conv_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Определение устройства\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "adj = adj.clone().detach().to(torch.float32).to(device) if isinstance(adj, torch.Tensor) else torch.tensor(adj, dtype=torch.float32).to(device)\n",
    "\n",
    "num_nodes = train_data.shape[1]\n",
    "input_dim = 1  # Например, только скорость\n",
    "hidden_dim = 64\n",
    "output_dim = 1\n",
    "num_layers = 4\n",
    "pre_len = 12\n",
    "space = 'r'\n",
    "\n",
    "# Конфигурация эмбеддингов\n",
    "emb_configs = {\n",
    "    1: (288, 5),    # Канал 1: 288 индексов в течение дня\n",
    "    # 2: (7, 5),      # Канал 2: 7 индексов в течение недели\n",
    "    # 3: (325, 5),    # Канал 3: 325 индексов узлов\n",
    "}\n",
    "\n",
    "# Обновление модели, данных и вычислений\n",
    "model = QGNNTrafficPredictor(adj, num_nodes, input_dim, hidden_dim, output_dim, num_layers, pre_len, space, emb_configs).to(device)\n",
    "criterion = nn.MSELoss()  # Функция потерь\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "\n",
    "# Получаем один батч данных\n",
    "train_iter = iter(train_loader)\n",
    "history_data = next(train_iter)[0]  # Получаем одну порцию данных из train_loader\n",
    "summary(model, input_data=history_data.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация TensorBoard и функция запуска обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(output, target):\n",
    "    \"\"\"Вычисление метрик MAE, RMSE, MAPE.\"\"\"\n",
    "    abs_error = torch.abs(output - target).sum().item()\n",
    "    mae = abs_error / len(target)\n",
    "    rmse = ((output - target) ** 2).sum().item() / len(target)\n",
    "    rmse = rmse ** 0.5\n",
    "    mape = (abs_error / torch.abs(target).sum().item()) if torch.abs(target).sum().item() != 0 else 0\n",
    "    return mae, rmse, mape\n",
    "\n",
    "def train_val_test_model(model, train_loader, val_loader, test_loader, epochs, writer):\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # === Тренировка ===\n",
    "        model.train()\n",
    "        train_loss, train_mae, train_rmse, train_mape = 0.0, 0.0, 0.0, 0.0\n",
    "        train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs} - Training\", leave=False)\n",
    "\n",
    "        for x, y in train_loader_tqdm:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x).squeeze(-1)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * x.size(0)\n",
    "\n",
    "            mae, rmse, mape = compute_metrics(output, y)\n",
    "            train_mae += mae\n",
    "            train_rmse += rmse\n",
    "            train_mape += mape\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_mae /= len(train_loader)\n",
    "        train_rmse /= len(train_loader)\n",
    "        train_mape /= len(train_loader)\n",
    "\n",
    "        writer.add_scalar(\"Loss/Train\", train_loss, epoch + 1)\n",
    "        writer.add_scalar(\"MAE/Train\", train_mae, epoch + 1)\n",
    "        writer.add_scalar(\"RMSE/Train\", train_rmse, epoch + 1)\n",
    "        writer.add_scalar(\"MAPE/Train\", train_mape, epoch + 1)\n",
    "\n",
    "        # === Валидация ===\n",
    "        model.eval()\n",
    "        val_loss, val_mae, val_rmse, val_mape = 0.0, 0.0, 0.0, 0.0\n",
    "        val_loader_tqdm = tqdm(val_loader, desc=f\"Epoch {epoch + 1}/{epochs} - Validation\", leave=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader_tqdm:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                output = model(x).squeeze(-1)\n",
    "                loss = criterion(output, y)\n",
    "                val_loss += loss.item() * x.size(0)\n",
    "\n",
    "                mae, rmse, mape = compute_metrics(output, y)\n",
    "                val_mae += mae\n",
    "                val_rmse += rmse\n",
    "                val_mape += mape\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_mae /= len(val_loader)\n",
    "        val_rmse /= len(val_loader)\n",
    "        val_mape /= len(val_loader)\n",
    "\n",
    "        writer.add_scalar(\"Loss/Validation\", val_loss, epoch + 1)\n",
    "        writer.add_scalar(\"MAE/Validation\", val_mae, epoch + 1)\n",
    "        writer.add_scalar(\"RMSE/Validation\", val_rmse, epoch + 1)\n",
    "        writer.add_scalar(\"MAPE/Validation\", val_mape, epoch + 1)\n",
    "\n",
    "        # === Тестирование ===\n",
    "        test_loss, test_mae, test_rmse, test_mape = 0.0, 0.0, 0.0, 0.0\n",
    "        test_loader_tqdm = tqdm(test_loader, desc=f\"Epoch {epoch + 1}/{epochs} - Testing\", leave=False)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader_tqdm:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                output = model(x).squeeze(-1)\n",
    "                loss = criterion(output, y)\n",
    "                test_loss += loss.item() * x.size(0)\n",
    "\n",
    "                mae, rmse, mape = compute_metrics(output, y)\n",
    "                test_mae += mae\n",
    "                test_rmse += rmse\n",
    "                test_mape += mape\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        test_mae /= len(test_loader)\n",
    "        test_rmse /= len(test_loader)\n",
    "        test_mape /= len(test_loader)\n",
    "\n",
    "        writer.add_scalar(\"Loss/Test\", test_loss, epoch + 1)\n",
    "        writer.add_scalar(\"MAE/Test\", test_mae, epoch + 1)\n",
    "        writer.add_scalar(\"RMSE/Test\", test_rmse, epoch + 1)\n",
    "        writer.add_scalar(\"MAPE/Test\", test_mape, epoch + 1)\n",
    "\n",
    "        # Сохранение лучшей модели\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f\"{writer.log_dir}best_model.pth\")\n",
    "\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обучение, валидация и тестирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация TensorBoard\n",
    "writer = SummaryWriter(log_dir=f\"runs/T-GCN/speed_GCN_D2/\")\n",
    "train_val_test_model(model, train_loader, val_loader, test_loader, epochs=50, writer=writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
